\documentclass{ieeeaccess}
\usepackage{cite}
\usepackage{amsmath,amssymb,amsfonts}
\usepackage{algorithmic}
\usepackage{graphicx}
\usepackage{textcomp}
\usepackage{algorithm2e}
\usepackage{multirow}
\usepackage{bm}
\def\BibTeX{{\rm B\kern-.05em{\sc i\kern-.025em b}\kern-.08em
    T\kern-.1667em\lower.7ex\hbox{E}\kern-.125emX}}
\begin{document}
%\history{Date of publication xxxx 00, 0000, date of current version xxxx 00, 0000.}
\history{}
%\doi{10.1109/ACCESS.2017.DOI}
\doi{}

\title{Recent Research Progress on Cognitive Capabilities of Service Robots}
\author{\uppercase{Hecheng Zhao}\authorrefmark{1},
\uppercase{Yuan Li\authorrefmark{2}, and Sukhan Lee}.\authorrefmark{3},
\IEEEmembership{Fellow, IEEE}}
\address[1]{Intelligent System Research Institute (ISRI),
Sungkyunkwan University, 16419, Suwon, South Korea (e-mail: hczhao@skku.edu)}
\address[2]{Benewake Co., Ltd., Beijing, China (ly@benewake.com)}
\address[3]{Intelligent System Research Institute (ISRI),
Sungkyunkwan University, 16419, Suwon, South Korea (e-mail:
lsh@ece.skku.ac.kr)}
%\tfootnote{This paragraph of the first footnote will contain support 
%information, including sponsor and financial support acknowledgment. For 
%example, ``This work was supported in part by the U.S. Department of 
%Commerce under Grant BS123456.''}

%\markboth
%{Author \headeretal: Preparation of Papers for IEEE TRANSACTIONS and JOURNALS}
%{Author \headeretal: Preparation of Papers for IEEE TRANSACTIONS and JOURNALS}

\corresp{Corresponding author: Sukhan Lee (e-mail: lsh@ece.skku.ac.kr).}

\begin{abstract}
This report summarizes the works which have been presented by the Intelligent Systems Research Institute of Sungkyunkwan University to the community of artificial intelligence and robotics. Five relevant articles published by the Institute during 2015 to 2018 are reviewed in order to exhibit the research progress made on cognitive capabilities of service robots. The Major Line Extraction algorithm has been proposed to exploit line features of the environments. This novel algorithm exhibits an improvement on extracting cursive lines with less discontinuities. Experimental results have shown that the algorithm outperforms a well-known method called the Line Segment Detector. Additionally, the Covariance Projection Filtering is a sensor filtering framework proposed for distributed sensory systems. The framework generalizes a filtering model for Gaussian linear systems. Comparing with Kalman filters, it provides an additional step for detecting data anomalies. For indoor environments, Wi-Fi fingerprints are of available landmarks for localizing robot positions. A new approach has been proposed based on the concept of "invariant received signal length statistics" that is able to overcome the instability of Wi-Fi signals. Experimental results show that this new method provides higher performance by 17\% in terms of success rate. For object detection, a new method has been proposed for detecting industrial objects with less textures. The method employees model-based point cloud matching within an adaptive Bayesian framework. Experiments show that such an approach achieves over 97.5\% of recognition rate. Later, a deep neural network has been integrated into such a framework. It adopts a Convolutional Neural Network based feature extraction and reconstruction model to handle a larger number of objects without performance degeneration. It turns out that such a method allows the number of objects and their categories to be extended by 10 and 5 times, respectively.
\end{abstract}

\begin{keywords}
Covariance Projection Filtering, Feature Extraction, Line Segment Detection, Object Recognition, Pose Estimation, Simultaneous Localization and Mapping, Semantic Understanding.
\end{keywords}

\titlepgskip=-15pt

\maketitle

\section{Major Line Extraction}
\label{sec:introduction}
\subsection{Summary}
The novelty of the proposed method\cite{7031358} lies in the framework
of recruiting major lines from the maximally generated zero
threshold Canny edge links with the Sobel highlights as a guide
for recruitment. This makes it easier not only for representing
lines as a whole in terms of connected edge clusters but also
for tracing cursive lines of a higher curvature along Sobel highlights, resulting in a better way of incorporating a wider scope of
global context and of optimizing the sensitivity and robustness
trade-off.

\subsection{Methodology}
To obtain the Sobel highlights from
an input image, $I_0$, and its min-max normalized Sobel image, $I_s$,
first, at each pixel, a $3\times n$ oblong shape mask is defined along the
direction of its Sobel edge orientation with the pixel at the center,
as shown in Figure \ref{oblong}

\Figure[t!](topskip=0pt, botskip=0pt, midskip=0pt){oblong.png}
{An oblong shape mask placed for a pixel.\label{oblong}}.

Major lines tend
to have their lengths sufficiently long. The longer the length,
the better the chance of the edge link is being a major line. The
length index, $w_l$, is defined by
\begin{equation}
w_l=log_N(number\ of\ the\ pixels\ of\ a\ link).
\end{equation}

The above three indices, $w_c$, $w_a$, and $w_l$, quantifying for the
potential of a zero-threshold Canny edge link as a major line,
are now combined into a single index, $w$, referred to here as the
recruitment index, as follows:
\begin{equation}
w=w_{l}w_{c}^{\alpha}w_{\alpha}^{\beta},
\end{equation}
where the parameters, $\alpha$ and $\beta$, are to be used for setting the
relative significance between $w_c$ and $w_a$.

Finally, a zero-threshold Canny edge link is selected as a
major line, if the recruitment index, $w$, is larger than the pre-set
threshold. Note that, in this letter, $\alpha$ and $\beta$ are both set as 0.5 as
the default values and the recruitment index threshold is set as
1.0. For more detailed flow of the proposed algorithm, refer to
the following pseudo-code.
\begin{algorithm}
\caption{L-K Algorithm: A Major Line Segment Extraction}
\SetAlgoLined
\textbf{Input}: An Original Image $L$\\
\textbf{Output}: Line Segment $L$ \\
$I_{S}\leftarrow SobleImage(I)$ \\
$I_{E}\leftarrow CannyEdgeImage(I)$ \\
$E\leftarrow EdgeLinking(I_E)$ \\
\For {$i=0$ to $Sizeof(E)$} {
	\If {$E_i < T_i$ (default: $T_i = 6$)} {
		Continue	
	}
	$B[Sizeof(E_i)][n]\leftarrow$Set Double Array ($n$: oblong mask size) \\
	\For {$j=0$ to $Sizeof(E_i)$} {
		$P_j0, P_j1, P_j2\leftarrow$3 groups of pixel in oblong mask \\
		$\sigma_1\leftarrow$Orientation $Stdev(P_j1)$ \\ 
		$\mu_0, \mu_1, \mu_2\leftarrow$Intensity $Average(P_j0, P_j1, P_j2)$ \\
		\If {$\sigma_1>\rho$ (default: $\rho=15.0$)} {
			Continue		
		}
		\If {not ($\mu_0<\mu_1<\mu_2$ or $\mu_2<\mu_1<\mu_0$)} {
			Continue		
		}
		\For {$k=0$ to $n$} {
			$B[j][k]\leftarrow$Accumulate Sobel highlight score		
		}
	}
	$w_L\leftarrow log_n Sizeof(E_i)$ \\
	$w_c\leftarrow \frac{Sizeof(B[i])}{Sizeof(E_i)}$ \\
	$w_a\leftarrow log_n\frac{Sumof(B[i])}{Sizeof(B[i])}$ \\
	$w\leftarrow w_Lw_{c}^{\alpha}w_{\alpha}^{\beta}$ (default: $\alpha=\beta=0.5$) \\
	\If {$w>T_2$ (default: $T_2=1.0$)} {
		$L_i\leftarrow$Line Segment	
	}
}
\end{algorithm}

\subsection{Results}
To evaluate the performance of the proposed method in comparison with LSD, a top performance major line detector currently available, a major line extraction experimentation is carried out with the test images taken directly from the laboratory,
as shown in Figure \ref{mle_result_a}.

%\Figure[t!](topskip=0pt, botskip=0pt, midskip=0pt){mle_result_a.png}
%{The results of major lines extracted by the proposed
%method.\label{mle_result_a}}

\begin{figure}[t!]
\centering
\includegraphics[width=0.4\textwidth]{mle_result_a.png}
\label{mle_result_a}
\caption{The results of major lines extracted by the proposed
method.}
\end{figure}

The indices computed for
the proposed method and LSD are tabulated in Table \ref{mle_table}. 

\begin{table}
\caption{The performance analysis of the proposed method and LSD with
the coverage and the coverage per major line indices.}

\label{mle_table}
\setlength{\tabcolsep}{3pt}
\centering
\begin{tabular}{|c|c|c|c|c|}
\hline
\multirow{2}{*}{Major Lines} & \multicolumn{2}{l|}{Coverage} & \multicolumn{2}{l|}{Coverage per Major Line} \\ \cline{2-5} 
                             & Proposed         & LSD        & Proposed                & LSD                \\ \hline
Straight                     &   69\%               &     46\%       &          41\%               &                   38\% \\ \hline
Curved                      &     89\%         &     79\%     &         38\%            &           11\% \\ \hline
Combined                      &    76\%       &     58\%     &      40\%             &               19\% \\ \hline
\end{tabular}
\label{tab1}
\end{table}

\section{The Covariance Projection Framework}
\subsection{Summary}
The proposed method\cite{cpf}, referred to here as the Covariance Projection (CP) method, provides an
unbiased and optimal solution in the sense of minimum mean square error (MMSE), if the projection
is based on the minimum weighted distance on the constraint manifold. The proposed method
not only offers a generalization of the conventional formula for handling constraints and data
inconsistency, but also provides a new insight into data fusion in terms of a geometric-algebraic
point of view.
\subsection{Methodology}
he proposed method first represents the probability of true states and measurements in the
extended space around the data from state predictions and sensor measurements, where the extended
space is formed by taking states and measurements as independent variables. Any constraints among
true states and measurements that should be satisfied are then represented as a constraint manifold
in the extended space. This is shown schematically in Figure \ref{cpf_manifold} for filtering as an example. Data fusion is accomplished by projecting the probability distribution of true states
and measurements onto the constraint manifold.

%\Figure[t!](topskip=0pt, botskip=0pt, midskip=0pt){cpf_manifold.png}
%{Probability of true states and measurements in the extended space around the data from
%state predictions and sensor measurements and constraint manifold.\label{cpf_manifold}}

\begin{figure}[t!]
  \centering 
  \includegraphics[width=0.4\textwidth]{cpf_manifold.png}
  \caption{Probability of true states and measurements in the extended space.}
  \label{cpf_manifold}
\end{figure}

More specifically, consider two mean estimates, $\hat{x}_1$ and $\hat{x}_2$, of the state $x\in \mathbb{R}^N$, with their
respective covariances as $P_1, P_1\in \mathbb{R}^N\times N$. Furthermore, the estimates are assumed to be correlated with
cross-covariance $P_{12}$. The mean estimates and their covariances together with their cross-covariance in
$\mathbb{R}^N$ are then transformed to an extended space of $\mathbb{R}^{2N}$ along with the linear constraint between the two estimates:
\begin{equation}
\hat{x}=\begin{bmatrix}
\hat{x}_1 \\ \hat{x}_2
\end{bmatrix},\ P=\begin{bmatrix}
P_1 & P_{12} \\ P_{12}^T & P_2
\end{bmatrix},\ C_1\hat{x}_1=C_2\hat{x}_2
\end{equation}

To find a point on the constraint manifold with minimum weighted distance, we apply the whitening transform (WT) defined as, $W=D^{-1/2}E^T$, where $D$ and $E$ are the eigenvalue and eigenvector matrices of $P$. Applying WT,
\begin{equation}
\hat{x}^W=W\hat{x},\ P^W=WPW^T,\ M^W=WM
\end{equation}

where the matrix $M = [C_1\ C_2]^T$ is the subspace of the constraint manifold. The probability
distribution is then orthogonally projected on the transformed manifold $M^W$ to satisfy the constraints
between the data sources in the transformed space. Inverse WT is applied to
obtain the fused mean estimate and covariance in the original space,
\begin{equation}
\label{xwprw}
\tilde{x}=W^{-1}P_r W\hat{x}
\end{equation}
\begin{equation}
\label{pwprprtwt}
\tilde{P}=W^{-1}P_r P_{r}^{T}W^{-T}
\end{equation}
where $P_r=M^W(M^{W^T}M^W)^{-1}M^{W^T}$ is the orthogonal projection matrix. Using the definition of various
components in (\ref{xwprw}) and (\ref{pwprprtwt}), a close form simplification can be obtained as,
\begin{equation}
\label{xmmtpmmtpx}
\tilde{x}=M(M^TP^{-1}M)^{-1}M^TP^{-1}\hat{x}
\end{equation}
\begin{equation}
\label{pmmtpmmt}
\tilde{P}=M(M^TP^{-1}M)^{-1}M^T
\end{equation}

Due to the projection in extended space of $\mathbb{R}^{2N}$, (\ref{xmmtpmmtpx}) and (\ref{pmmtpmmt}) provide a fused result with respect to each data source. In the case of where $\hat{x}_1$ and $\hat{x}_2$ estimate the same entity, that is, $M=[I_N\ I_N]^T$, the fused result will be same for the two data sources. As such, a close form equation for fusing redundant data sources in $\mathbb{R}^N$ can be obtained from (\ref{xmmtpmmtpx}) and (\ref{pmmtpmmt}) as,
\begin{equation}
\tilde{x}=(M^TP^{-1}M)^{-1}M^TP^{-1}\hat{x}
\end{equation}
\begin{equation}
\tilde{P}=(M^TP^{-1}M)^{-1}
\end{equation}
\subsection{Results}
The simulation is carried out for 1000 Monte Carlo runs and the local estimates provided by four
sensor nodes along with the fused result of the CP method are shown in Figure \ref{cpf_result_image}. The straight lines
in Figure \ref{cpf_result_image} denote the trace of error covariance matrices and the solid curve represents the MSE of
local and fused estimates. It can be observed from Figure \ref{cpf_result_image} that the MSE of the individual sensor node fluctuates around the trace. Furthermore, the accuracy relation of
local sensor estimates and fused estimates in terms of MSE in Figure \ref{cpf_result_image} is coincident with the
theoretical result in Table \ref{cpf_result_table}.

\begin{figure}[t!]
\centering
\caption{The mean square error (MSE) and trace ($P_i$).}
\label{cpf_result_image}
\includegraphics[width=0.4\textwidth]{cpf_result_image.png}
\end{figure}

\begin{table}
\centering
\begin{tabular}{lllll}
\hline
\multirow{2}{*}{\textbf{Methods}} & \multicolumn{4}{l}{\textbf{Average RMSE}} \\ \cline{2-5} 
                  &     \textbf{$x_1$ (m)} & \textbf{$x_2$ (m)}     &  \textbf{$\dot{x}_1$ (m/s)}   &  \textbf{$\dot{x}_2$ (m/s)}  \\ \hline
             Unconstrained     &   95.479  &   87.496  &   52.413  &  49.972  \\
               CP   &  50.39   &   29.093  &  36.624   &  21.145  \\
               EP(I)   &   55.823  &  32.229   &  39.648   &  22.891  \\
               EP(${P^{u^{-1}}}$)   &  53.023   &   30.613  &   37.186  &  21.469  \\ \hline
\end{tabular}
\label{cpf_result_table}
\caption{Average RMSE for 1000 Monte Carlo Runs.}
\end{table}

\section{Wi-Fi Fingerprinting Based Indoor Localization}
\subsection{Summary}
The instability of Wi-Fi received signal strength (RSS) incurred
by mutable channel characteristics hampers a wide-spread
adoption of RSS based location fingerprinting to real world indoor
localization applications. To overcome RSS instability, we
propose a new approach based on the concept of “invariant RSS
statistics”. By invariant RSS statistics, we mean the RSS samples
collected at each calibration location, especially, under minimal
random spatiotemporal disturbances. The proposed method\cite{Husen:2016:HPI:2857546.2857589} forms
the reference pattern classes for individual calibration locations
with the invariant RSS statistics thus obtained. Fingerprinting is
done by identifying the reference pattern class that maximally
supports the RSS readings collected at an unknown location for
available Wi-Fi sources. The support of RSS readings is defined
here as the sum of the likelihood probabilities of individual RSS
readings. Unlike conventional methods, the proposed method
allows only those readings high in statistical confidence to
participate in the sum, while excluding other readings. This is to
screen out the influence of those readings contaminated by timevarying disturbances on classification. Experimental results show
that the proposed method provides superior performance to
conventional ones with the success rate higher by 17\%, the
printing resolution finer by 30\% and, naturally, no performance
degradation in time without recalibration.

\subsection{Methodology}
The $m$ dimensional Received Signal Strength (RSS) vector,
$\{\mathbf{s}_i(t)\}_j$, at time $t$ at a particular calibration location $j$ due to m Wi-Fi sources, $i=1, …, m$, is represented as
\begin{equation}
\{\mathbf{s}_i(t)\}_j=\{\mathbf{s}_{1,j}(t), \mathbf{s}_{2,j}(t),...,\mathbf{s}_{i,j},...,\mathbf{s}_{m,j}(t)\},\ j=1,...,n
\end{equation}

where the bold-face letter, $\mathbf{s}$, is to represent it as a random
variable.

Here, we model $\mathbf{s}_{i,j}(t)$, the RSS from the $i^{th}$ Wi-Fi source at the $j^{th}$
calibration location, as
\begin{equation}
\mathbf{s}_{i,j}(t)=\bm{\alpha}_{i,j}(t)\times r_{i,j}+\bm{\delta}_{i,j}
\end{equation}
where $r_{i,j}$ represents the time-invariant RSS with no
spatiotemporal disturbances present, $\bm{\alpha}_{i,j}(t)$ the multiplicative
signal alteration factor due to the spatiotemporal disturbances of
$r_{i,j}$, and $\bm{\sigma}_{i,j}$ the sensor noise. 

\subsection{Results}
The location of the experimentation is in the $6^{th}$ floor of Research
Complex 2, Sungkyunkwan University, South Korea. The
experimentation area is approximately 350 square meters, which
consists of a few rooms and hallway. We preset seven calibration
locations to begin with (we add more calibration locations as the
research progresses), where it is the spot that has been used to
collect the readings of the Wi-Fi RSS data. Figure \ref{wifi_setup} depicts the
floor map of the experimental area with 14 calibration locations
(Loc. 1 to Loc. 14).

\begin{figure}[t!]
\centering
\label{wifi_setup}
\includegraphics[width=0.5\textwidth]{wifi_setup.png}
\caption{The experiment floor map area with marked
calibration locations}
\end{figure}

The result as depicted in Figure \ref{wifi_result} shows that after 18 weeks, the
conventional approach degrades 14\% in success rate, while our
approach maintains the identical performance. In between week
12 and 13, we recalibrate location fingerprint of the conventional
approach and create a new database out of it, at the same time we
still keep the older database. After week 12, we observed that our
approach still conserve high performance results. On the contrary,
the success rate of the older location fingerprint of conventional
approach continues to deteriorate further. As for the new location
fingerprint after recalibration of the conventional approach,
although the success rate rise to 77\%, it is still significantly lower
than the proposed approach, and continues to deteriorate in the
following weeks. We conclude that our approach performs not
only better in performance, but also stable as time goes by.

\begin{figure*}[t!]
\centering
\label{wifi_result}
\includegraphics[width=0.7\textwidth]{wifi_result.png}
\caption{Time-based performance comparison with
recalibration applied to conventional approach.}
\end{figure*}

Table \ref{wifi_table} illustrates the comparison results to the existing
approaches. We observed that our invariant RSS approach
produced a higher success rate and stable regardless of the number
of samples over different length of time. On the contrary, the
conventional approach and the remove useless Wi-Fi sources
approach suffers from success rate degradation as the number
of samples increased over time. From our analysis, the reason
behind this result is our approach only takes invariant RSS which
maintain low variance although as time increased, as opposed to
existing approaches that takes all available RSS which increased
the variance by longer time.

\begin{table}
\label{wifi_table}
\setlength{\tabcolsep}{3pt}
\centering
\begin{tabular}{c | c c c}
\hline
 & Invariant RSS & Remove Useless Source & Conventional \\
\hline
 n=100 / (2 hours) & 92 & 73 & 71 \\
 n=200 / (1 week) & 94 & 68 & 62 \\
 n=300 / (2 weeks) & 91 & 58 & 52 \\
\hline
\end{tabular}
\caption{Performance comparison with different number of
samples collected over different time length.}
\end{table}

\section{Adaptive Bayesian Recognition}
\subsection{Summary}
Visual recognition and pose estimation of 3D
industrial objects are challenging due to variations of geometric
shapes from perspectives while lacking textures. We propose an approach\cite{7750717} to accurately recognizing industrial
objects and estimating their poses based on an adaptive
Bayesian framework with optimal feature selection and
model-based point cloud matching. First, feature sets that
provide sufficient evidences for recognition are selected from a
predefined pool of features based on the t-test. Then, the
evidence structure for Bayesian framework is updated
adaptively to the feature sets selected for recognition. Finally, a
process of coarse-fine pose estimation, composed of 3D feature
based coarse registration followed by Octree-ICP based model
matching for precision registration, is presented.
\subsection{Methodology}
Proposed framework mainly consists of two parts: 1) 3D
object recognition with multiple evidences. 2) model-based
3D object pose estimation. In recognition process, candidate
evidences include global and local features are extracted from
RGB-D data, i.e. 3D SIFT, CLB and shape descriptor, which
describe object’s texture and general shape. After
collecting candidate features, we apply t-test based feature
selection to choose sufficient features for different target
object as supporting evidences. Lastly, the measured features
are compared with each object in database using Bayesian
network. The object with high probability is considered as the
recognition result. For recognized object, 3D line registration
and 3D SIFT are used for the initial pose estimation. If both
are available, the one with higher confidence would be
adopted based on their registration accuracy. Then,
Octree-ICP is employed to get the refined pose estimation.

\begin{figure*}[t!]
\centering
\includegraphics[width=0.6\textwidth]{bayesian_framework.png}
\caption{Framework overview}
\end{figure*}

%After detecting CLB, we could get the number and their
%position in 3D space. To describe object using CLB, we would
%build a statistical model including CLB’s number and relative
%positions between CLBs based on different view-point data.
%For single object, there can exists more than one CLB,
%however, it is impossible to observe all the surface of an object
%at the same time. We have to estimate the CLB feature existing
%in one frame data. Regarding CLB’s number, it is not difficult
%to understand this model. For relative positions, we would
%store all the observed distances between any two detected
%CLB in a list for each object and set the detection accuracy as
%the variance.

The object probability consists of two parts:
\begin{equation}
F(CLB;u_{num},\Sigma_{num})=e^{-\frac{1}{2}(x-\mu)^T\Sigma^{-1}(x-\mu)}
\end{equation}
\begin{equation}
F(CLB;u_{dis},\Sigma_{dis})=max\ e^{-\frac{1}{2}(x-\mu)^T\Sigma^{-1}(x-\mu)}%,\ \mu\in u_{dis},\ \Sigma\in\Sigma_{dis}
\end{equation}
\begin{equation}
F(CLB)=F(CLB;u_{num},\Sigma_{num})*F(CLB;u_{dis},\Sigma_{dis})
\end{equation}
where $u_{num}$, $\Sigma_{num}$ is the mean and variance of number of
matched CLB for target object; $u_{dis}$ stored all the observed
distances between any two detected CLB in object surface,
while $\Sigma_{dis}$ is the measurement accuracy of CLB.

For measured features, we will apply a Gaussian function
to derive object sufficiency probability. By repeated
measurements, we could calculate the uncertainty existing in
each 3D shape descriptor, and has a better understanding of
this feature’s stability under measurement environment.

Prior probability:
\begin{equation}
F(shape\ descriptor;\mu_{obj},\Sigma_{obj})=\frac{1}{c}e^{-\frac{1}{2}(x-\mu)^T\Sigma^{-1}(x-\mu)}
\end{equation}

where $\mu$, $\Sigma$ is the mean and variance of specific shape
descriptor.
\subsection{Results}
Using robot arm, we move or rotate each target object 10
times, then capture the point cloud for pose estimation. Since
robot arm can offer object’s position as ground truth, the
object’s location from pose estimation is calculated and
compared.

\begin{table}
\setlength{\tabcolsep}{3pt}
\centering
\begin{tabular}{| c | c | c | c | c | c | c | c |}
\hline
\multirow{2}{*}{\textbf{Object}} & \multirow{2}{*}{\textbf{error}} & X  & Y & Z & rX & rY  & rZ  \\
 & & (mm) & (mm) & (mm) & (Deg) & (Deg) & (Deg) \\
\hline
\textbf{mouse}  & mean & 2.472 & 0.632 & 1.912 & 0.556 & 0.148 & 0.513 \\ \cline{2-8}
\textbf{part 1} & Std & 0.943 & 1.672 & 0.552 & 0.442 & 0.197 & 0.199 \\
\hline
\textbf{mouse} & mean & 0.232 & 1.368 & 0.379 & 0.252 & 0.122 & 0.118 \\ \cline{2-8}
\textbf{part 2} & Std & 0.66 & 0.617 & 0.903 & 0.164 & 0.062 & 0.161 \\
\hline
\textbf{mouse} & mean & 0.174 & 0.097 & 0.151 & 0.111 & 0.037 & 0.148 \\ \cline{2-8}
\textbf{part 3} & Std & 0.133 & 0.039 & 0.141 & 0.107 & 0.049 & 0.042 \\
\hline
\textbf{Baby} & mean & 0.316 & 0.645 & 0.06 & 1.526 & 1.327 & 0.984 \\ \cline{2-8}
\textbf{Control PCB} & Std & 1.357 & 0.34 & 0.293 & 1.299 & 0.227 & 0.202 \\ 
\hline
\textbf{Projector} & mean & 0.226 & 0.356 & 0.053 & 0.283 & 0.055 & 0.48 \\ \cline{2-8}
\textbf{Control PCB} & Std & 0.212 & 0.151 & 0.385 & 0.165 & 0.246 & 0.181 \\
\hline
\multirow{2}{*}{\textbf{Clamp}} & mean & 0.446 & 0.416 & 0.172 & 0.524 & 1.645 & 1.614 \\ \cline{2-8}
& Std & 0.067 & 0.441 & 0.185 & 1.233 & 0.521 & 1.254 \\
\hline
\multirow{2}{*}{\textbf{Mater}} & mean & 0.26 & 0.221 & 0.184 & 0.327 & 0.836 & 0.971 \\ \cline{2-8}
& Std & 0.074 & 0.259 & 0.686 & 0.737 & 0.718 & 0.277 \\
\hline
\multirow{2}{*}{\textbf{Saw}} & mean & 0.679 & 0.621 & 1.073 & 0.382 & 1.138 & 1.567 \\ \cline{2-8}
& Std & 0.791 & 0.245 & 0.097 & 1.44 & 1.632 & 0.531 \\
\hline
\textbf{Alternater} & mean & 0.937 & 0.194 & 0.126 & 0.026 & 0.156 & 0.345 \\ \cline{2-8}
\textbf{Cover} & Std & 0.419 & 0.45 & 0.109 & 0.036 & 0.141 & 0.148 \\
\hline
\textbf{Base} & mean & 0.437 & 0.494 & 0.192 & 0.227 & 0.316 & 0.04 \\ \cline{2-8}
\textbf{Board} & Std & 0.039 & 0.107 & 0.009 & 0.039 & 0.101 & 0.109 \\
\hline
\end{tabular}
\caption{Performance comparison with different number of
samples collected over different time length.}
\end{table}

\begin{figure}[t!]
\centering
\includegraphics[width=0.4\textwidth]{bayesian_result.png}
\caption{Captured 3D point clouds of PCB and base board.}
\end{figure}

\section{3D Deep Object Recognition and Semantic Understanding}
\subsection{Summary}
For the success of visually-guided robotic errand
service, it is critical to ensure dependability under various
ill-conditioned visual environments. To this end, we have
developed Adaptive Bayesian Recognition Framework in which
in-situ selection of multiple sets of optimal features or evidences
as well as proactive collection of sufficient evidences are
proposed to implement the principle of dependability. The
framework\cite{8593985} has shown excellent performance with a limited
number of objects in a scene. However, there arises a need to
extend the framework for handling a larger number of objects
without performance degradation, while avoiding difficulty in
feature engineering. To this end, a novel deep learning
architecture, referred to here as FER-CNN, is introduced and
integrated into the Adaptive Bayesian Recognition Framework.
FER-CNN has capability of not only extracting but also
reconstructing a hierarchy of features with the layer-wise
independent feedback connections that can be trained.
Reconstructed features representing parts of 3D objects then
allow them to be semantically linked to ontology for exploring
object categories and properties. Experiments are conducted in a
home environment with real 3D daily-life objects as well as with
the standard ModelNet dataset.
\subsection{Methodology}

For the automatic extraction and reconstruction of a
hierarchy of features, we propose a new CNN architecture
with layer-wise inverse connections, termed as Feature
Extraction and Reconstruction CNN (FER-CNN), as shown in
Figure \ref{deep_architecture}. The FER-CNN implemented consists of 3
sub-networks: Encoder, Decoder and Classifier. Encoder and
Decoder sub-networks are mirrored in their configurations but
with independent weights assigned to their respective
convolutional and deconvolutional connections. Encoder
sub-network consists of five 3D convolutional layers, each
followed by the batch normalization and the ReLu activation
function. Each of the five 3D deconvolutional layer of
Decoder sub-network accepts an input from either of the
following three sources: the corresponding encoder layer in
the form of a skip connection, the upper deconvolutional layer,
or the externally injected latent code. Classification
sub-network, consisting of 3 fully-connected layers with a
softmax layer at the last, takes the concatenated dense feature
output of all encoder layers as an input.

\begin{figure}[t!]
\centering
\label{deep_architecture}
\includegraphics[width=0.5\textwidth]{deep_architecture.png}
\caption{Architecture of FER-CNN for HomeMate Robot}
\end{figure}

The training of FER-CNN follows the following 3 phases
in general:

Phase 1: Supervised end-to-end training of the
encoder and classification sub-networks using cross
entropy loss.

Phase 2: Unsupervised training of the decoder weights
of each layer from layer 1 to 5 using the following loss
function with the weights of Encoder sub-network
either fixed or tunable:

\begin{equation}
\begin{split}
\min_{E_i,D_i}( & (x-D_{i\sim 1}(E_{1\sim i(x)}))^2 \\ & + (E_{1\sim i}(x) - E_{1\sim i}(D_{i\sim 1}(E_{1\sim i})))^2)
\end{split}
\end{equation}
\begin{equation}
\begin{split}
\min_{E_{1\sim i},D_{1\sim i}}( & (x-D_{i\sim 1}(E_{1\sim i}(x)))^2 \\ & + (E_{1\sim i}(x) - E_{1\sim i}(D_{i\sim 1}(E_{1\sim i}(x))))^2)
\end{split}
\end{equation}

As far as reconstruction of a specific feature is concerned,
it is not trivial as a receptive field of the input space is not only
a function of the corresponding feature of a particular layer but
also of its neighboring features of that layer, referred to here as
a response field, that affect the receptive field. This happens due to the overlapping convolution
windows dictated by the choice of layer configurations with
window size and stride. Taking the above response field into
consideration, we present the following, so called,
contribution-based trained reconstruction algorithm:

\begin{algorithm}
\caption{Contribution-based trained reconstruction algorithm}
\SetAlgoLined
\textbf{Input}: feature "f" at location "p" in layer "L" \\
\textbf{Output}: reconstructed receptive field in input space "x" \\
1. Identify "Rx" receptive field of "p" in input space \\
2. Identify “Rf” response field in layer “L” by adding the
response field of each cell in “Rx” \\
3. Copy “Rf” from a training sample in which it has the
closest response to “f” at location “p” in layer “L” \\
4. Overwrite the value of location “p” in “Rf” with “f” \\
5. Reconstruct “x*” in input space from layer “L” using “Rf” padded with zeros \\
6. Crop “x*” using receptive field “Rx” to obtain “x” \\ \ \\
\end{algorithm}

\subsection{Results}
To evaluate our extended framework, we trained
FER-CNN on ISRI\_DB consisting of RGB and 3D point
cloud data of 247 household objects in 33 categories. The 3D
point cloud of each object is a full-blown model generated by
registering a number of data from different viewpoints. Based
on the 3D point cloud models stored in the DB, we generated
1,000 random poses per object as training samples. For
testing, we first placed about 10 real objects defined in
ISRI\_DB in a cluttered setting on a table. And, then, all the
objects on the table were labeled automatically from a single
view of RGB-D data for scene understanding. Figure \ref{deep_result} (top)
illustrates an example of 3D region proposals based on the
combination of 3D octree segmentation and 2D region
proposal of Faster RCNN. 
We repeated the experiments 10 times, each with roughly 15
3D objects per scene. The average precision of FER-CNN for
classification is about 91\%. Then, the object pose was
estimated first roughly by regressing a fully connected layer
with the features of FER-CNN and then refined by ICP,
resulting in about 10 degrees of orientation error.

\begin{figure}[t!]
\centering
\label{deep_result}
\includegraphics[width=0.4\textwidth]{deep_result.png}
\caption{Region proposals obtained using Faster R-CNN}
\end{figure}

FER-CNN is evaluated further using a large-scale of
ModelNet dataset. ModelNet has two datasets: ModelNet10

having 10 classes of 48000 3D CAD models and ModelNet40
having 40 classes of 151,128 3D CAD models. We used two
fully connected layers along with one dropout layer at the
output of the encoder sub-network of FER-CNN without
resorting to conventional 3D shape descriptors. We trained
FER-CNN on ModelNet10 and ModelNet40 datasets and
achieved 97\% accuracy on ModelNet10 testing dataset and
89\% accuracy on ModelNet40 testing datasets, as shown in
Table \ref{deep_table}.

\begin{table}
\label{deep_table}
\setlength{\tabcolsep}{3pt}
\centering
\begin{tabular}{|c|c|c|}
\hline
 & ModelNet10 & ModelNet40 \\
Model & Classification test & Classification test \\
& accuracy & accuracy \\
\hline
VoxNet & 92\% & 83\% \\
\hline
FER-CNN (Ours) & 97\% & 89.5\% \\
\hline
\end{tabular}
\caption{Categorization accuracy on ModelNet dataset.}
\end{table}

\section{Conclusion}
In recent years, the Intelligent System and Robotics Institute has pushed the boundary of cognitive robotics in object recognition and SLAM fields. The Major Line Extractor has improved localization accuracy for indoor navigation. The novel algorithm has been applied to a number of real world applications. One of them is the robotic vacuum cleaners developed by LG Electronics. Additionally, the Wi-Fi Fingerprint becomes one of the auxiliary means that improve indoor SLAM, especially in solving "Robot Kidnapping" problems. The Covariance Projection Framework offers a new view of Gaussian linear filters, which facilitates the research on state estimation with multi-sensory setups. Moreover, the Institute has also made progress on object detection and semantic understanding. The Adaptive Bayesian Recognition enables accurate recognition and pose estimation on industrial objects without salient textures. With the integration of FER-CNN, we managed to extend the number of recognizable objects and their categories to a higher level.

\bibliography{mybib}
\bibliographystyle{IEEETran}

\begin{IEEEbiography}[{\includegraphics[width=1in,height=1.25in,clip,keepaspectratio]{me.png}}]{Hecheng Zhao} received the B.S. degree
in Automation from
the Nanjing University of Posts and Telecommunications,
Nanjing, China, in 2015. He is currently pursuing the M.S. degree in AI and Robotics with Sungkyunkwan University,
Suwon, South Korea.
His research interests include SLAM, machine learning, and computer vision.
\end{IEEEbiography}

\begin{IEEEbiography}[{\includegraphics[width=1in,height=1.25in,clip,keepaspectratio]{lee.png}}]{Sukhan Lee}  received the B.S. and M.S. degrees
in electrical engineering from Seoul National
University, South Korea, in 1972 and 1974, respectively, and the Ph.D. degree in electrical engineering from Purdue University, West Lafayette, IN,
USA, in 1982.
From 1983 to 1997, he was with the Departments of Electrical Engineering and of Computer
Science, University of Southern California. From
1990 to 1997, he was with the Jet Propulsion
Laboratory, California Institute of Technology, as a Senior Member of the
Technical Staff. From 1998 to 2003, he was the Executive Vice President,
and also the Chief Research Officer with the Samsung Advanced Institute
of Technology. He has been working as a Professor of information and
communication engineering and WCU Professor of interaction science with
Sungkyunkwan University, since 2003. He was designated the Dean of the
Graduate School of Sungkyunkwan University, in 2011.He is also serving
as the Director of the Intelligent Systems Research Institute. His research
interests are in the areas of cognitive robotics, intelligent systems, and
micro/nano electro-mechanical systems.
Dr. Lee is currently a Fellow of the Korean National Academy of Science
and Technology.
\end{IEEEbiography}

\EOD

\end{document}
